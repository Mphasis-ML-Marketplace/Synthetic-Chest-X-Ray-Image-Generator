{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Chest X-Ray Image Generator \n",
    "\n",
    "This solution generates synthetic Chest X-ray images for educational and research purposes by leveraging state of the art generative AI techniques in images. \n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to For Seller to update: Arrhythmia Identification from ECG. \n",
    "\n",
    "#### Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Output Result](#D.-Output-Result)\n",
    "   5. [Delete the endpoint](#E.-Delete-the-endpoint)\n",
    "3. [Perform batch inference](#3.-Perform-batch-inference) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "#### Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the model package listing page Synthetic-chest-X-ray-image-generator.\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_arn='Synthetic-chest-X-ray-image-generator'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json \n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import urllib.request\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sage.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, see [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Synthetic-xray-generator\"\n",
    "\n",
    "content_type = \"application/zip\"\n",
    "\n",
    "real_time_inference_instance_type = \"ml.g4dn.8xlarge\"\n",
    "batch_transform_inference_instance_type = \"ml.p3.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wrapper(endpoint, session):\n",
    "    return sage.predictor.RealTimePredictor(endpoint, session, content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a deployable model from model package\n",
    "model = ModelPackage(role=role,\n",
    "                    model_package_arn=model_package_arn, \n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    predictor_cls=predict_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: Synthetic-chest-X-ray-image-generator-2023-11-03-12-27-44-330\n",
      "INFO:sagemaker:Creating endpoint-config with name Synthetic-xray-generator\n",
      "INFO:sagemaker:Creating endpoint with name Synthetic-xray-generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(1, real_time_inference_instance_type, endpoint_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "    1) The payload should be in zip format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"user_input.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"ContentType\": \"application/zip\",\r\n",
      "    \"InvokedProductionVariant\": \"AllTraffic\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name $model_name --body fileb://$file_name --content-type 'application/zip' --region us-east-2 output.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Output Result\n",
    "\n",
    "- The output file (in zip format) contains the following files:\n",
    "\n",
    "    1. 'output.zip': A folder containing different synthetic chest X-ray images generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from zipfile import *\n",
    "\n",
    "file_path = os.getcwd()\n",
    "file_name_ = 'output.zip'\n",
    "\n",
    "file_object = open(file_name_,'rb')\n",
    "z = ZipFile(file_object)\n",
    "file_names = []\n",
    "for name in z.namelist():\n",
    "    z.extract(name,file_path)\n",
    "    file_names.append(name)\n",
    "file_object.close()\n",
    "\n",
    "output_folder_path = os.path.join(file_path, \"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Delete the endpoint\n",
    "\n",
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: Synthetic-xray-generator\n",
      "INFO:sagemaker:Deleting endpoint with name: Synthetic-xray-generator\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together. If you are not familiar with batch transform, and want to learn more, see these links:\n",
    "1. [How it works](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-batch-transform.html)\n",
    "2. [How to run a batch transform job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform input uploaded to s3://sagemaker-us-east-2-786796469737/synthetic-chest-X-ray-generator/batch/user_input.zip\n"
     ]
    }
   ],
   "source": [
    "#upload the batch-transform job input files to S3\n",
    "transform_input_folder = file_name\n",
    "batch_input_prefix = \"synthetic-chest-X-ray-generator/batch\"\n",
    "transform_input = sagemaker_session.upload_data(transform_input_folder, key_prefix=batch_input_prefix) \n",
    "print(\"Transform input uploaded to \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: Synthetic-chest-X-ray-image-generator-2023-11-03-12-09-04-952\n",
      "INFO:sagemaker:Creating transform job with name: Synthetic-chest-X-ray-image-generator-2023-11-03-12-09-05-821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................\u001b[34m==========\u001b[0m\n",
      "\u001b[34m== CUDA ==\u001b[0m\n",
      "\u001b[34m==========\u001b[0m\n",
      "\u001b[34mCUDA Version 11.7.1\u001b[0m\n",
      "\u001b[34mContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[0m\n",
      "\u001b[34mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[35m==========\u001b[0m\n",
      "\u001b[35m== CUDA ==\u001b[0m\n",
      "\u001b[35m==========\u001b[0m\n",
      "\u001b[35mCUDA Version 11.7.1\u001b[0m\n",
      "\u001b[35mContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[0m\n",
      "\u001b[35mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[34mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[34mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[34mA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\u001b[0m\n",
      "\u001b[34m2023-11-03 12:17:27.877776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[0m\n",
      "\u001b[34mTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[35mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[35mA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\u001b[0m\n",
      "\u001b[35m2023-11-03 12:17:27.877776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[0m\n",
      "\u001b[35mTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-11-03 12:17:28.979986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[0m\n",
      "\u001b[35m2023-11-03 12:17:28.979986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[0m\n",
      "\u001b[34m * Serving Flask app 'serve'\n",
      " * Debug mode: off\u001b[0m\n",
      "\u001b[34m#033[31m#033[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.#033[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://169.254.255.131:8080\u001b[0m\n",
      "\u001b[34m#033[33mPress CTRL+C to quit#033[0m\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [03/Nov/2023 12:17:30] \"GET /ping HTTP/1.1\" 200 -\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [03/Nov/2023 12:17:30] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[34mtrue\u001b[0m\n",
      "\u001b[34mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip']\u001b[0m\n",
      "\u001b[34m<_io.BufferedWriter name='/opt/program/user_input.zip'>\u001b[0m\n",
      "\u001b[34mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip', 'user_input']\u001b[0m\n",
      "\u001b[34minside inference: 20\u001b[0m\n",
      "\u001b[34mInside generate recons\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mGlobal seed set to 0\u001b[0m\n",
      "\u001b[35m * Serving Flask app 'serve'\n",
      " * Debug mode: off\u001b[0m\n",
      "\u001b[35m#033[31m#033[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.#033[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://169.254.255.131:8080\u001b[0m\n",
      "\u001b[35m#033[33mPress CTRL+C to quit#033[0m\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [03/Nov/2023 12:17:30] \"GET /ping HTTP/1.1\" 200 -\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [03/Nov/2023 12:17:30] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[35mtrue\u001b[0m\n",
      "\u001b[35mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip']\u001b[0m\n",
      "\u001b[35m<_io.BufferedWriter name='/opt/program/user_input.zip'>\u001b[0m\n",
      "\u001b[35mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip', 'user_input']\u001b[0m\n",
      "\u001b[35minside inference: 20\u001b[0m\n",
      "\u001b[35mInside generate recons\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[35mGlobal seed set to 0\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\u001b[0m\n",
      "\u001b[34mdata preprocessing done!!\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mGPU available: True (cuda), used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mbefore trainer.predict\u001b[0m\n",
      "\u001b[35m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\u001b[0m\n",
      "\u001b[35mdata preprocessing done!!\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[35mGPU available: True (cuda), used: True\u001b[0m\n",
      "\u001b[35mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[35mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[35mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[35mbefore trainer.predict\u001b[0m\n",
      "\u001b[34mMissing logger folder: /opt/program/output/lightning_logs\u001b[0m\n",
      "\u001b[35mMissing logger folder: /opt/program/output/lightning_logs\u001b[0m\n",
      "\u001b[32m2023-11-03T12:17:30.315:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[35mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\u001b[0m\n",
      "\u001b[34m#015Predicting: 0it [00:00, ?it/s]preprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting:   0%|          | 0/20 [00:00<?, ?it/s]#015Predicting DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\u001b[0m\n",
      "\u001b[35m#015Predicting: 0it [00:00, ?it/s]preprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting:   0%|          | 0/20 [00:00<?, ?it/s]#015Predicting DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Predicting DataLoader 0:  15%|█▌        | 3/20 [01:10<06:42, 23.65s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  15%|█▌        | 3/20 [01:10<06:42, 23.65s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  20%|██        | 4/20 [01:34<06:16, 23.53s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  20%|██        | 4/20 [01:34<06:16, 23.53s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  25%|██▌       | 5/20 [01:57<05:51, 23.46s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  25%|██▌       | 5/20 [01:57<05:51, 23.46s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  35%|███▌      | 7/20 [02:43<05:04, 23.39s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  35%|███▌      | 7/20 [02:43<05:04, 23.39s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  40%|████      | 8/20 [03:06<04:40, 23.37s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  40%|████      | 8/20 [03:06<04:40, 23.37s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  45%|████▌     | 9/20 [03:30<04:16, 23.35s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  45%|████▌     | 9/20 [03:30<04:16, 23.35s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  50%|█████     | 10/20 [03:53<03:53, 23.33s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  50%|█████     | 10/20 [03:53<03:53, 23.33s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  55%|█████▌    | 11/20 [04:16<03:29, 23.31s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  55%|█████▌    | 11/20 [04:16<03:29, 23.31s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  65%|██████▌   | 13/20 [05:02<02:43, 23.29s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  65%|██████▌   | 13/20 [05:02<02:43, 23.29s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  70%|███████   | 14/20 [05:25<02:19, 23.28s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  70%|███████   | 14/20 [05:25<02:19, 23.28s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  75%|███████▌  | 15/20 [05:48<01:56, 23.27s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  75%|███████▌  | 15/20 [05:48<01:56, 23.27s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  80%|████████  | 16/20 [06:12<01:33, 23.26s/it]In predict_step module\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  80%|████████  | 16/20 [06:12<01:33, 23.26s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  85%|████████▌ | 17/20 [06:35<01:09, 23.25s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  85%|████████▌ | 17/20 [06:35<01:09, 23.25s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  95%|█████████▌| 19/20 [07:21<00:23, 23.24s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  95%|█████████▌| 19/20 [07:21<00:23, 23.24s/it]In predict_step module\u001b[0m\n",
      "\u001b[34msynthetic images folder name: ['images']\u001b[0m\n",
      "\u001b[34msynthetic images generated: ['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[34m['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[34mSynthetic images generated\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [03/Nov/2023 12:25:19] \"POST /invocations HTTP/1.1\" 200 -\u001b[0m\n",
      "\u001b[35msynthetic images folder name: ['images']\u001b[0m\n",
      "\u001b[35msynthetic images generated: ['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[35m['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[35mSynthetic images generated\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [03/Nov/2023 12:25:19] \"POST /invocations HTTP/1.1\" 200 -\u001b[0m\n",
      "\n",
      "\u001b[34m==========\u001b[0m\n",
      "\u001b[34m== CUDA ==\u001b[0m\n",
      "\u001b[34m==========\u001b[0m\n",
      "\u001b[34mCUDA Version 11.7.1\u001b[0m\n",
      "\u001b[34mContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[0m\n",
      "\u001b[34mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[35m==========\u001b[0m\n",
      "\u001b[35m== CUDA ==\u001b[0m\n",
      "\u001b[35m==========\u001b[0m\n",
      "\u001b[35mCUDA Version 11.7.1\u001b[0m\n",
      "\u001b[35mContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\u001b[0m\n",
      "\u001b[35mThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\u001b[0m\n",
      "\u001b[34mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[34mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[34mA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\u001b[0m\n",
      "\u001b[34m2023-11-03 12:17:27.877776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[0m\n",
      "\u001b[34mTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[35mBy pulling and using the container, you accept the terms and conditions of this license:\u001b[0m\n",
      "\u001b[35mhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\u001b[0m\n",
      "\u001b[35mA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\u001b[0m\n",
      "\u001b[35m2023-11-03 12:17:27.877776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\u001b[0m\n",
      "\u001b[35mTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-11-03 12:17:28.979986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[0m\n",
      "\u001b[35m2023-11-03 12:17:28.979986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\u001b[0m\n",
      "\u001b[34m * Serving Flask app 'serve'\n",
      " * Debug mode: off\u001b[0m\n",
      "\u001b[34m#033[31m#033[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.#033[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://169.254.255.131:8080\u001b[0m\n",
      "\u001b[34m#033[33mPress CTRL+C to quit#033[0m\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [03/Nov/2023 12:17:30] \"GET /ping HTTP/1.1\" 200 -\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [03/Nov/2023 12:17:30] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[34mtrue\u001b[0m\n",
      "\u001b[34mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip']\u001b[0m\n",
      "\u001b[34m<_io.BufferedWriter name='/opt/program/user_input.zip'>\u001b[0m\n",
      "\u001b[34mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip', 'user_input']\u001b[0m\n",
      "\u001b[34minside inference: 20\u001b[0m\n",
      "\u001b[34mInside generate recons\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mGlobal seed set to 0\u001b[0m\n",
      "\u001b[35m * Serving Flask app 'serve'\n",
      " * Debug mode: off\u001b[0m\n",
      "\u001b[35m#033[31m#033[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.#033[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://169.254.255.131:8080\u001b[0m\n",
      "\u001b[35m#033[33mPress CTRL+C to quit#033[0m\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [03/Nov/2023 12:17:30] \"GET /ping HTTP/1.1\" 200 -\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [03/Nov/2023 12:17:30] \"#033[33mGET /execution-parameters HTTP/1.1#033[0m\" 404 -\u001b[0m\n",
      "\u001b[35mtrue\u001b[0m\n",
      "\u001b[35mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip']\u001b[0m\n",
      "\u001b[35m<_io.BufferedWriter name='/opt/program/user_input.zip'>\u001b[0m\n",
      "\u001b[35mlist the dir: ['vae.py', 'ddpm.py', 'spaced_diff_form2.py', 'ddpm_form2.py', 'wrapper.py', 'serve', 'model', 'test.yaml', 'Inference.py', 'callbacks.py', 'spaced_diff.py', 'unet_openai.py', 'user_input.zip', 'user_input']\u001b[0m\n",
      "\u001b[35minside inference: 20\u001b[0m\n",
      "\u001b[35mInside generate recons\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/seed.py:48: LightningDeprecationWarning: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[35mGlobal seed set to 0\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\u001b[0m\n",
      "\u001b[34mdata preprocessing done!!\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[34mGPU available: True (cuda), used: True\u001b[0m\n",
      "\u001b[34mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[34mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[34mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[34mbefore trainer.predict\u001b[0m\n",
      "\u001b[35m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 14513.16it/s]\u001b[0m\n",
      "\u001b[35mdata preprocessing done!!\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\u001b[0m\n",
      "\u001b[35mGPU available: True (cuda), used: True\u001b[0m\n",
      "\u001b[35mTPU available: False, using: 0 TPU cores\u001b[0m\n",
      "\u001b[35mIPU available: False, using: 0 IPUs\u001b[0m\n",
      "\u001b[35mHPU available: False, using: 0 HPUs\u001b[0m\n",
      "\u001b[35mbefore trainer.predict\u001b[0m\n",
      "\u001b[34mMissing logger folder: /opt/program/output/lightning_logs\u001b[0m\n",
      "\u001b[35mMissing logger folder: /opt/program/output/lightning_logs\u001b[0m\n",
      "\u001b[32m2023-11-03T12:17:30.315:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[35mLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\u001b[0m\n",
      "\u001b[34m#015Predicting: 0it [00:00, ?it/s]preprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34m#015Predicting:   0%|          | 0/20 [00:00<?, ?it/s]#015Predicting DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\u001b[0m\n",
      "\u001b[35m#015Predicting: 0it [00:00, ?it/s]preprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting:   0%|          | 0/20 [00:00<?, ?it/s]#015Predicting DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:   5%|▌         | 1/20 [00:24<07:47, 24.62s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:   5%|▌         | 1/20 [00:24<07:47, 24.62s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  10%|█         | 2/20 [00:47<07:09, 23.89s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  10%|█         | 2/20 [00:47<07:09, 23.89s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  15%|█▌        | 3/20 [01:10<06:42, 23.65s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  15%|█▌        | 3/20 [01:10<06:42, 23.65s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  20%|██        | 4/20 [01:34<06:16, 23.53s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  20%|██        | 4/20 [01:34<06:16, 23.53s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  25%|██▌       | 5/20 [01:57<05:51, 23.46s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  25%|██▌       | 5/20 [01:57<05:51, 23.46s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  30%|███       | 6/20 [02:20<05:27, 23.42s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  30%|███       | 6/20 [02:20<05:27, 23.42s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  35%|███▌      | 7/20 [02:43<05:04, 23.39s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  35%|███▌      | 7/20 [02:43<05:04, 23.39s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  40%|████      | 8/20 [03:06<04:40, 23.37s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  40%|████      | 8/20 [03:06<04:40, 23.37s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  45%|████▌     | 9/20 [03:30<04:16, 23.35s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  45%|████▌     | 9/20 [03:30<04:16, 23.35s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  50%|█████     | 10/20 [03:53<03:53, 23.33s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  50%|█████     | 10/20 [03:53<03:53, 23.33s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  55%|█████▌    | 11/20 [04:16<03:29, 23.31s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  55%|█████▌    | 11/20 [04:16<03:29, 23.31s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  60%|██████    | 12/20 [04:39<03:06, 23.30s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  60%|██████    | 12/20 [04:39<03:06, 23.30s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  65%|██████▌   | 13/20 [05:02<02:43, 23.29s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  65%|██████▌   | 13/20 [05:02<02:43, 23.29s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  70%|███████   | 14/20 [05:25<02:19, 23.28s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  70%|███████   | 14/20 [05:25<02:19, 23.28s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  75%|███████▌  | 15/20 [05:48<01:56, 23.27s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  75%|███████▌  | 15/20 [05:48<01:56, 23.27s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mpreprocess reference image (64, 64, 3)\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  80%|████████  | 16/20 [06:12<01:33, 23.26s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  80%|████████  | 16/20 [06:12<01:33, 23.26s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  85%|████████▌ | 17/20 [06:35<01:09, 23.25s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  85%|████████▌ | 17/20 [06:35<01:09, 23.25s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  90%|█████████ | 18/20 [06:58<00:46, 23.24s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  90%|█████████ | 18/20 [06:58<00:46, 23.24s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0:  95%|█████████▌| 19/20 [07:21<00:23, 23.24s/it]In predict_step module\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0:  95%|█████████▌| 19/20 [07:21<00:23, 23.24s/it]In predict_step module\u001b[0m\n",
      "\u001b[34mwrite the reconstructed images\u001b[0m\n",
      "\u001b[34m#015Predicting DataLoader 0: 100%|██████████| 20/20 [07:44<00:00, 23.23s/it]#015Predicting DataLoader 0: 100%|██████████| 20/20 [07:44<00:00, 23.23s/it]\u001b[0m\n",
      "\u001b[34msynthetic images folder name: ['images']\u001b[0m\n",
      "\u001b[34msynthetic images generated: ['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[34m['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[34mSynthetic images generated\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [03/Nov/2023 12:25:19] \"POST /invocations HTTP/1.1\" 200 -\u001b[0m\n",
      "\u001b[35mwrite the reconstructed images\u001b[0m\n",
      "\u001b[35m#015Predicting DataLoader 0: 100%|██████████| 20/20 [07:44<00:00, 23.23s/it]#015Predicting DataLoader 0: 100%|██████████| 20/20 [07:44<00:00, 23.23s/it]\u001b[0m\n",
      "\u001b[35msynthetic images folder name: ['images']\u001b[0m\n",
      "\u001b[35msynthetic images generated: ['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[35m['output_ddpm_0_4_0.png', 'output_ddpm_0_3_0.png', 'output_ddpm_0_12_0.png', 'output_ddpm_0_10_0.png', 'output_ddpm_0_15_0.png', 'output_ddpm_0_1_0.png', 'output_ddpm_0_18_0.png', 'output_ddpm_0_7_0.png', 'output_ddpm_0_16_0.png', 'output_ddpm_0_8_0.png', 'output_ddpm_0_9_0.png', 'output_ddpm_0_13_0.png', 'output_ddpm_0_19_0.png', 'output_ddpm_0_17_0.png', 'output_ddpm_0_11_0.png', 'output_ddpm_0_14_0.png', 'output_ddpm_0_2_0.png', 'output_ddpm_0_6_0.png', 'output_ddpm_0_0_0.png', 'output_ddpm_0_5_0.png']\u001b[0m\n",
      "\u001b[35mSynthetic images generated\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [03/Nov/2023 12:25:19] \"POST /invocations HTTP/1.1\" 200 -\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Run the batch inference job\n",
    "transformer = model.transformer(1, batch_transform_inference_instance_type)\n",
    "transformer.transform(transform_input, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-786796469737/Synthetic-chest-X-ray-image-generator-2023-11-03-12-09-05-821'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: Synthetic-chest-X-ray-image-generator-2023-11-03-12-09-04-952\n"
     ]
    }
   ],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the algorithm, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
